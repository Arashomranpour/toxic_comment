## Screenshots:

model accuracy
![image](https://github.com/user-attachments/assets/2bbe3a5f-ab4c-473e-97b2-b59d018c7a28)

metrics with plot
![image](https://github.com/user-attachments/assets/ba9a972b-d8f9-4dae-9cb7-fa589e2daa61)
![image](https://github.com/user-attachments/assets/b43e48cf-3c43-48b3-9c23-1b9640417477)

## Toxic Comment Classification
This repository contains a project focused on identifying and classifying toxic comments using machine learning and natural language processing (NLP) techniques. The objective is to develop a model that can accurately identify various types of toxic language within user-generated content.

## Project Overview
The toxic_comment_classification project is built on a dataset containing a variety of user comments labeled for different types of toxicity, such as insults, threats, or hate speech. The project aims to build a classifier that can categorize these comments, supporting content moderation efforts and enhancing user safety.

## Contents
toxic_comment_classification.ipynb: The primary notebook for the project, containing data preprocessing, exploratory data analysis, model training, evaluation, and results.
## Project Workflow
Data Preprocessing: Handling missing values, text cleaning, and preparing the data for model training.
Exploratory Data Analysis (EDA): Visualizing and understanding the distribution of toxicity levels across the dataset.
Model Selection and Training: Experimenting with various NLP models and tuning hyperparameters to maximize performance.
Evaluation and Results: Evaluating model performance using metrics like accuracy, F1-score, precision, and recall.


## Usage
Open the toxic_comment_classification.ipynb notebook.
Follow the cells to preprocess data, train models, and evaluate results.
## Results
The final model achieved a satisfactory performance level in detecting and classifying toxic comments, making it suitable for real-world applications like content moderation.
